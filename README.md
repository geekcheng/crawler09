crawler09
=========

主要功能模块:
=============
                     |-数据抓取端，数据抓取端采用phantomjs，链接在这里http://phantomjs.org/。
          crawler09- |-数据持久化，数据库采用mysql，本来使用mongodb，但是和solr整合时出现问题，正在解决。
                     |-抓取控制，采用nodejs子进程调用外部程序进行控制。
                     |-数据检索，mysql结合solr，进行数据检索。
抓取大体流程：
=============
              抓取控制端从数据库查询出若干种子链接，启动子进程（@1）调用外部程序数据抓取端进行种子链接数据的抓取，
          数据抓取端进行页面元素的解析和获取，之后组装成一个json数组，包括要传送到后台进行处理的数据（@2）。
          对传送到后台的数据进行解码（@3）等操作之后，把数据进行持久化。
          数据的检索，本系统使用的是solrj，使用spring框架，进行数据的索引创建，并打算实现更多复杂的操作。
实用价值：
=========
          可以进行数据的定向抓取，也可以结合需求进行简单的数据分析，好吧，更响亮的名字可以是数据挖掘。
使用的技术：
===========
          在抓取端使用的是phantomjs，支持dom操作。
          逻辑控制主要使用的是nodejs，数据库链接使用的是native-mysql。
          抓取配置主要使用的是spring。
要进行的改进：
=============
          1.使抓取和解析模版分离，以使同样的逻辑能够适应更多的抓取模版.
          2.抓取控制的界面化操作和精确控制，进一步实现抓取的控制，比如使用nodejs监听子进程来替换setInterval。
          3.数据检索编程使用spring操作solrj，实现更多的功能，和可配置型。
          4.数据持久化的优化操作，比如使用完成后关闭连接，更可以使用缓存来减轻数据库并发的读写压力。
          5.代码的重构，在抓取和逻辑控制中使用了很多具有相同功能的函数，可以进一步进行优化，同时可以把很多代码写到一个模块，以提高重用性。






